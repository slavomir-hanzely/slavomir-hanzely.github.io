@article{Mishchenko2021,
 abstract = {First-Order Model-Agnostic Meta-Learning (FO-MAML) has been studied as inexact SGD applied to the objective of standard MAML. In this work, we go beyond this perspective and present a convergence theory that shows fast convergence of FO-MAML to the vicinity of a solution of a different objective. Namely, we study a more general family of first-order algorithms for meta-learning with the objective defined as the sum of Moreau envelopes over all tasks. Our theory has several advantages over the inexact SGD framework; in particular, we show that FO-MAML can minimize the implicit meta-learning loss despite using a finite number of gradient steps only. Moreover, our perturbed-iterate analysis allows for tighter guarantees that improve the dependency on the problem's conditioning. In contrast to the related work, ours does not require any assumptions on the Hessian smoothness, and can leverage smoothness and convexity of the reformulation based on Moreau envelopes. Furthermore, to fill the gaps in the comparison to the Implicit MAML (iMAML), we show that the objective of iMAML is neither smooth nor convex, implying that it has no convergence guarantees based on the existing theory. We corroborate our theoretical findings with numerical experiments on neural networks.},
 author = {Konstanin Mishchenko and Slavomír Hanzely and Peter Richtárik},
 title = {Convergence of First-Order Algorithms for Meta-Learning with Moreau Envelopes},
 year = {2021}
}

